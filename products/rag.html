<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>RAG.io — Enterprise-grade RAG platform with multi-provider LLM support, document intelligence, and real-time streaming</title>
  <link rel="stylesheet" href="../assets/css/style.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
  <style>
      .logo-img {
      height: 120px;
      width: auto;
      object-fit: contain;
      }
</style>
</head>

<body>
<div class="container">
<header class="site-header">
  <nav>
    <a href="../index.html" class="logo">        
      <img src="../assets/img/ragio.png" alt="A&ECoding Logo" class="logo-img" />
      </a>
    <ul>
      <li><a href="../index.html">Home</a></li>
      <li><a href="theai.html">THEAI.io</a></li>
      <li><a href="llm.html">LLM.io</a></li>
      <li><a href="../index.html#contact">Contact</a></li>
    </ul>
  </nav>
</header>

<!-- HERO -->
<section class="hero">
  <h1>RAG.io</h1>
  <p class="subtitle">
    Retrieval-Augmented Generation.
  </p>

  <div class="hero-cta">
    <a href="#" class="btn-primary">Presentation</a>
    <a href="https://github.com/iwebbo/RAG.io" class="btn-secondary" target="_blank">Code source</a>
  </div>
</section>

<!-- PRESENTATION -->
<section class="hero-glass">
  <h2>Build with</h2>
    <div class="tech-icons">
    <i class="fa-brands fa-docker" title="Docker"></i>
    <i class="fa-brands fa-linux" title="Linux"></i>
    <i class="fa-brands fa-python" title="Python"></i>
    <i class="fa-brands fa-github" title="Github"></i>
    <i class="fa-brands fa-react" title="React"></i>
  </div>
</section>

<section class="products" aria-label="Nos applications principales">
  <article class="product-card" id="theai" tabindex="0">
    <h2>Current Features</h2>
    <ul class="features-list">
      <li><strong>Intelligent Document Search :</strong> ChromaDB-powered semantic search with adjustable retrieval parameters</li>
      <li><strong>Multi-Provider LLM Support :</strong> Seamless integration with OpenAI, Claude, Gemini, Ollama, and 10+ providers</li>
      <li><strong>Temperature Control :</strong> Per-conversation adjustment (0.0-2.0)</li>
      <li><strong>Project-Based Organization :</strong> Isolate document collections and conversations by project</li>
      <li><strong>Real-Time Streaming :</strong> Server-Sent Events for progressive responses</li>
      <li><strong>Fine-Grained Control :</strong> Per-conversation temperature, top-k, and context window management	</li>
      <li><strong>Enterprise Security :</strong> JWT authentication, AES-256 encryption, GDPR compliance</li>
    </ul>
  </article>

  <article class="product-card" id="theai" tabindex="0">
    <h2>Advanced Features</h2>
    <ul class="features-list">
      <li><strong>Supported Formats :</strong> PDF, DOCX, TXT, MD, HTML, CSV, JSON (50+ file types)</li>
      <li><strong>Smart Chunking :</strong> Adaptive chunk size (100-2000 tokens) with configurable overlap</li>
      <li><strong>Metadata Extraction :</strong> Automatic filename, page number, and document type tagging</li>
      <li><strong>Token Tracking :</strong> Real-time token counting for cost estimation</li>
      <li><strong>Batch Processing :</strong> Background async processing with progress tracking</li>
    </ul>
  </article>

  <article class="product-card" id="theai" tabindex="0">
    <h2>Providers Supported</h2>
    <ul class="features-list">
      <li><strong>OpenAI :</strong> GPT-4o, GPT-4-turbo, o1-preview, o1-mini.</li>
      <li><strong>Anthropic Claude :</strong> Claude 3.5 Sonnet, Claude 3 Opus</li>
      <li><strong>Google Gemini :</strong> Gemini 1.5 Pro/Flash, Gemini 2.0</li>
      <li><strong>OpenRouter :</strong> 200+ models (free + paid)</li>
      <li><strong>xAI Grok :</strong> Grok-3, Grok-3-mini, Grok-3-vision</li>
      <li><strong>Groq :</strong> 	Mixtral, LLaMA 3, Gemma</li>
      <li><strong>HuggingFace :</strong> Zephyr, Mistral, LLaMA 2</li>
      <li><strong>Ollama :</strong> ollama pull llama3</li>
      <li><strong>LM Studio :</strong> GUI app</li>
      <li><strong>vLLM :</strong> Python + CUDA</li>
      <li><strong>LMDeploy :</strong> Python + TurboMind</li>
      <li><strong>Oobabooga :</strong> Web UI</li>
    </ul>
  </article>
</section>

  <section class="about" aria-label="Présentation de aecoding.io">
    <h2>About</h2>
      <p>This project is licensed under the MIT License - see the LICENSE file for details.</p>
  </section>

  <section class="about" aria-label="Contact">
    <h2>Contact me</h2>
    <p>Built with ❤️ for the LLM community</p>
    <p>For questions, suggestions, or support, please open an issue or contact the maintainers.<a href="mailto:l.kieran95@gmail.com">@me</a>.</p>
  </section>


<footer>
  <p>© 2025 aecoding.io — Tous droits réservés.</p>
</footer>
</div>
</body>
</html>
